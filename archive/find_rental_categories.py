#!/usr/bin/env python3
"""
101evler.com'daki TÃœM KÄ°RALIK KATEGORÄ°LERÄ°NÄ° BUL
==============================================

Bu script, 101evler.com sitesindeki tÃ¼m kiralÄ±k emlak kategorilerini bulur.
"""

import asyncio
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import re

async def find_rental_categories():
    """101evler.com'dan tÃ¼m kiralÄ±k kategorileri bul"""
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘   101evler.com KÄ°RALIK KATEGORÄ°LER                        â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    
    async with AsyncWebCrawler() as crawler:
        print("ğŸ“¡ 101evler.com ana sayfasÄ± Ã§ekiliyor...")
        result = await crawler.arun(
            url='https://www.101evler.com',
            use_playwright=True
        )
        
        if not result or not result.html:
            print("âŒ Sayfa Ã§ekilemedi!")
            return
        
        print("âœ… Sayfa Ã§ekildi, parse ediliyor...")
        soup = BeautifulSoup(result.html, 'html.parser')
        
        # TÃ¼m linkleri bul
        all_links = soup.find_all('a', href=True)
        
        # KiralÄ±k kategorilerini filtrele
        rental_categories = {}
        
        for link in all_links:
            href = link['href']
            text = link.get_text(strip=True)
            
            # /kibris/kiralik-XXX formatÄ±ndaki linkleri bul
            if '/kibris/kiralik-' in href:
                # Kategori ismini Ã§Ä±kar (kiralik-XXX)
                match = re.search(r'/kibris/(kiralik-[\w-]+)', href)
                if match:
                    category = match.group(1)
                    if category not in rental_categories:
                        rental_categories[category] = {
                            'url': href,
                            'text': text or 'N/A'
                        }
        
        # Kategorileri sÄ±rala ve gÃ¶ster
        print()
        print("ğŸ  BULUNAN KÄ°RALIK KATEGORÄ°LER:")
        print("="*60)
        
        sorted_categories = sorted(rental_categories.items())
        
        for idx, (category, info) in enumerate(sorted_categories, 1):
            print(f"{idx:2d}. {category:30s} | {info['text']}")
        
        print("="*60)
        print(f"Toplam {len(rental_categories)} kategori bulundu")
        print()
        
        # Åehirler iÃ§in Ã¶rnekler
        print("ğŸ“ ÅEHÄ°RLER:")
        print("   - lefkosa, girne, magusa, gazimagusa, iskele, guzelyurt")
        print()
        
        # KullanÄ±m Ã¶rnekleri
        print("ğŸ’¡ KULLANIM Ã–RNEÄÄ°:")
        print("   docker-compose run --rm scraper \\")
        print("     bash -c 'sed -i \"s/^PROPERTY_TYPE = .*/PROPERTY_TYPE = \\\"kiralik-daire\\\"/\" src/scraper/config.py && \\")
        print("              python -m scraper.main'")
        print()
        
        return rental_categories

if __name__ == '__main__':
    asyncio.run(find_rental_categories())
