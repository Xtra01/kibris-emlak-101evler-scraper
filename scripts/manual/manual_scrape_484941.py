#!/usr/bin/env python3
"""
484941 ilanÄ±nÄ± MANUEL olarak Ã§ek ve kaydet
"""

import asyncio
from crawl4ai import AsyncWebCrawler
import os

async def manual_scrape():
    """484941 ilanÄ±nÄ± manuel Ã§ek"""
    
    target_url = "https://www.101evler.com/kibris/kiralik-emlak/girne-lapta-daire-484941.html"
    output_file = "listings/484941.html"
    
    print("="*70)
    print("ğŸ¯ MANUEL SCRAPE - 484941")
    print("="*70)
    print(f"\nHedef URL: {target_url}")
    print(f"Ã‡Ä±ktÄ±: {output_file}\n")
    
    async with AsyncWebCrawler() as crawler:
        print("ğŸŒ Sayfa Ã§ekiliyor (Playwright ile)...")
        try:
            result = await crawler.arun(
                url=target_url,
                bypass_cache=True,
                js_code="""
                // SayfanÄ±n tamamen yÃ¼klenmesini bekle
                await new Promise(r => setTimeout(r, 2000));
                """
            )
            
            if result and result.html:
                html = result.html
                print(f"âœ… HTML alÄ±ndÄ± ({len(html)} karakter)")
                
                # Kaydet
                os.makedirs("listings", exist_ok=True)
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(html)
                
                print(f"ğŸ’¾ Dosya kaydedildi: {output_file}")
                
                # Ä°Ã§erik kontrolÃ¼
                if "484941" in html:
                    print("âœ… Ä°lan ID doÄŸrulandÄ±")
                if "Lapta" in html:
                    print("âœ… Konum doÄŸrulandÄ±")
                if "KiralÄ±k" in html or "Kiralik" in html:
                    print("âœ… Tip doÄŸrulandÄ±")
                    
            else:
                print("âŒ HTML alÄ±namadÄ±!")
                
        except Exception as e:
            print(f"âŒ HATA: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*70)
    print("âœ… Manuel scrape tamamlandÄ±!")
    print("="*70)

if __name__ == "__main__":
    asyncio.run(manual_scrape())
